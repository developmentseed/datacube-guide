{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b44dbc99",
   "metadata": {},
   "source": [
    "# Tiny data chunks\n",
    "\n",
    "There are three primary reasons why you should avoid using too small of chunk sizes in your datacubes:\n",
    "\n",
    "- Inefficient compression since most compression algorithms leverage correlations within a chunk.\n",
    "- Inefficient data loading when querying large subsets of the data cube due to numerous GET requests with high latency. The excessive GET requests also increases costs.\n",
    "- Inefficient decompression due to the number of chunks greatly exceeding available parallelism.\n",
    "\n",
    "Please note that issue of too many GET requests can be mitigated but not completely solved by using Zarr V3 sharding or a cloud-native file format that allows storing multiple chunks in a single file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a0edb6-7220-4910-854d-3b07d4e4f417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datacube_benchmark\n",
    "import obstore as obs\n",
    "import zarr\n",
    "import pandas as pd\n",
    "import hvplot.pandas  # noqa\n",
    "from pint import Quantity\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from obstore.auth.azure import AzureCredentialProvider\n",
    "\n",
    "_ = zarr.config.set({\"async.concurrency\": 128})\n",
    "pd.set_option(\"display.float_format\", \"{:0.30f}\".format)\n",
    "\n",
    "credential_provider = AzureCredentialProvider(credential=DefaultAzureCredential())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63dc2a66-c5df-4017-84d8-fb26e78e6a72",
   "metadata": {},
   "source": [
    "## Demonstrating storage inefficiencies of too small of chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc53811b-8177-4acb-a1f8-7723df8ca60c",
   "metadata": {},
   "source": [
    "Create a blosc compressed array with 25 KB chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b29b1e5-21f0-4732-9be7-3dd653e67478",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_store = obs.store.AzureStore.from_url(\n",
    "    \"https://datacubeguide.blob.core.windows.net/performance-testing/zarr-tiny-chunks\",\n",
    "    credential_provider=credential_provider,\n",
    ")\n",
    "tiny_chunks_zarr_store = datacube_benchmark.create_zarr_store(\n",
    "    object_store,\n",
    "    compressor=zarr.codecs.BloscCodec(\n",
    "        cname=\"zstd\", clevel=3, shuffle=zarr.codecs.BloscShuffle.shuffle\n",
    "    ),\n",
    "    target_chunk_size=\"25 kilobyte\",\n",
    "    target_array_size=\"10 GB\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92cfd511-82b8-4448-a4cf-bdff680ea35c",
   "metadata": {},
   "source": [
    "Create a blosc compressed array with 25 MB chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d29ede92-5643-45ba-aa22-ddcd09575dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "object_store = obs.store.AzureStore.from_url(\n",
    "    \"https://datacubeguide.blob.core.windows.net/performance-testing/zarr-reg-chunks\",\n",
    "    credential_provider=credential_provider,\n",
    ")\n",
    "reg_chunks_zarr_store = datacube_benchmark.create_zarr_store(\n",
    "    object_store,\n",
    "    compressor=zarr.codecs.BloscCodec(\n",
    "        cname=\"zstd\", clevel=3, shuffle=zarr.codecs.BloscShuffle.shuffle\n",
    "    ),\n",
    "    target_chunk_size=\"25 megabyte\",\n",
    "    target_array_size=\"10 GB\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f682c59-d7f3-4e6e-b9e0-1115470798b2",
   "metadata": {},
   "source": [
    "Compare the storage size of the two arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47421ce8-c62d-4baa-b342-0145fc7c351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_chunks_array = zarr.open_array(tiny_chunks_zarr_store, zarr_version=3, path=\"data\")\n",
    "tiny_chunks_storage_size = Quantity(\n",
    "    datacube_benchmark.utils.array_storage_size(tiny_chunks_array), \"bytes\"\n",
    ").to(\"GB\")\n",
    "tiny_chunks_compression_ratio = (\n",
    "    tiny_chunks_array.nbytes / tiny_chunks_storage_size.to(\"bytes\").magnitude\n",
    ")\n",
    "reg_chunks_array = zarr.open_array(reg_chunks_zarr_store, zarr_version=3, path=\"data\")\n",
    "reg_chunks_storage_size = Quantity(\n",
    "    datacube_benchmark.utils.array_storage_size(reg_chunks_array), \"bytes\"\n",
    ").to(\"GB\")\n",
    "reg_chunks_compression_ratio = (\n",
    "    reg_chunks_array.nbytes / reg_chunks_storage_size.to(\"bytes\").magnitude\n",
    ")\n",
    "\n",
    "print(\"Storage size of a 10 GB array in object storage:\")\n",
    "print(f\"\\t25 KB chunks: {tiny_chunks_storage_size:.2f}\")\n",
    "print(f\"\\t25 MB chunks: {reg_chunks_storage_size:.2f}\")\n",
    "print(\"Compression ratio of a 10 GB array in object storage:\")\n",
    "print(f\"\\t25 KB chunks: {tiny_chunks_compression_ratio:.2f}\")\n",
    "print(f\"\\t25 MB chunks: {reg_chunks_compression_ratio:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96a14c2-87e5-46a6-9c73-ab4f57ca8998",
   "metadata": {},
   "source": [
    "Notice the much better compression ratio for a datacube with 25 MB chunks relative to a datacube with 25 KB chunks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c333c5c-d53b-402b-a1d6-887c2c0760fc",
   "metadata": {},
   "source": [
    "## Demonstrating performance inefficiencies of too small of chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd21c382-ce9d-4d27-917f-bef6fc4231cd",
   "metadata": {},
   "source": [
    "Test the time required to load a random point, a time series, or a spatial slice for the array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb59f08-774d-401e-a1bd-3df77dceb727",
   "metadata": {},
   "outputs": [],
   "source": [
    "tiny_chunks_results = datacube_benchmark.benchmark_access_patterns(\n",
    "    tiny_chunks_array, num_samples=10\n",
    ").reset_index(drop=True)\n",
    "reg_chunks_results = datacube_benchmark.benchmark_access_patterns(\n",
    "    reg_chunks_array, num_samples=10\n",
    ").reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4ec267-f265-4723-a624-50ca881aef80",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([tiny_chunks_results, reg_chunks_results])\n",
    "df[\"access_pattern\"] = df[\"access_pattern\"].replace(\n",
    "    {\n",
    "        \"point\": \"Random point\",\n",
    "        \"time_series\": \"Time series\",\n",
    "        \"spatial_slice\": \"Spatial slice\",\n",
    "        \"full\": \"Full scan\",\n",
    "    }\n",
    ")\n",
    "df[\"mean_time\"] = df.apply(lambda row: float(row[\"mean_time\"].magnitude), axis=1)\n",
    "df[\"chunk_size\"] = df.apply(lambda row: f\"{row['chunk_size'].magnitude:,.2f}\", axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "044ad79b-3c56-4cc9-968c-08e04b443007",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Duration to load data for difference access patterns\"\n",
    "plt = df.hvplot.bar(\n",
    "    x=\"chunk_size\",\n",
    "    y=\"mean_time\",\n",
    "    by=\"access_pattern\",\n",
    "    width=1000,\n",
    "    rot=45,\n",
    "    title=title,\n",
    "    ylabel=\"Duration (s)\",\n",
    "    xlabel=\"Chunk Size, Query type\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3823e4a-9bca-46a9-b627-a6ea47025809",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33980fb9-b6a5-4f04-a868-989035bc06dd",
   "metadata": {},
   "source": [
    "Note that while random point access is faster for datacubes with smaller chunks, the time for loading many chunks is dramatically worse."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
