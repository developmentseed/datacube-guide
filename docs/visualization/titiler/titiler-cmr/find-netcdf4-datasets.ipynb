{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89c26d2e",
   "metadata": {},
   "source": [
    "# Finding NetCDF-4 collections\n",
    "\n",
    "This notebook shows how to use earthaccess to discover NASA Earthdata collections that provide granules in netCDF-4 format. In the next step, it opens a representative netCDF-4 file from each collection to inspect and list the available variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2105efe7-6e6c-4d1c-a713-b0ceabd40ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from typing import Dict, Optional, Tuple, Any\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helpers to parse metadata from earthaccess\n",
    "# ----------------------------------------\n",
    "\n",
    "\n",
    "def _parse_temporal(umm: Dict[str, Any]) -> Tuple[Optional[str], Optional[str]]:\n",
    "    temporal = umm.get(\"TemporalExtents\", [])\n",
    "    rng = (temporal or [{}])[0].get(\"RangeDateTimes\", [])\n",
    "    begin = (rng or [{}])[0].get(\"BeginningDateTime\")\n",
    "    end = (rng or [{}])[0].get(\"EndingDateTime\")\n",
    "    return begin, end\n",
    "\n",
    "\n",
    "def _parse_bounds_from_spatial(\n",
    "    umm: Dict[str, Any],\n",
    ") -> Tuple[Optional[float], Optional[float], Optional[float], Optional[float]]:\n",
    "    spatial = umm.get(\"SpatialExtent\", {}) or {}\n",
    "    horiz = spatial.get(\"HorizontalSpatialDomain\", {}) or {}\n",
    "    geom = horiz.get(\"Geometry\", {}) or {}\n",
    "\n",
    "    # 1) Bounding rectangles\n",
    "    rects = geom.get(\"BoundingRectangles\") or []\n",
    "    if rects:\n",
    "        wests = [r.get(\"WestBoundingCoordinate\") for r in rects if r]\n",
    "        easts = [r.get(\"EastBoundingCoordinate\") for r in rects if r]\n",
    "        souths = [r.get(\"SouthBoundingCoordinate\") for r in rects if r]\n",
    "        norths = [r.get(\"NorthBoundingCoordinate\") for r in rects if r]\n",
    "        if all(len(lst) > 0 for lst in (wests, easts, souths, norths)):\n",
    "            return (\n",
    "                float(np.min(wests)),\n",
    "                float(np.min(souths)),\n",
    "                float(np.max(easts)),\n",
    "                float(np.max(norths)),\n",
    "            )\n",
    "\n",
    "    # 2) GPolygons\n",
    "    gpolys = geom.get(\"GPolygons\") or []\n",
    "    coords_w, coords_e, coords_s, coords_n = [], [], [], []\n",
    "    for gp in gpolys:\n",
    "        b = gp.get(\"Boundary\", {})\n",
    "        pts = b.get(\"Points\", [])\n",
    "        lons = [p.get(\"Longitude\") for p in pts if p and p.get(\"Longitude\") is not None]\n",
    "        lats = [p.get(\"Latitude\") for p in pts if p and p.get(\"Latitude\") is not None]\n",
    "        if lons and lats:\n",
    "            coords_w.append(np.min(lons))\n",
    "            coords_e.append(np.max(lons))\n",
    "            coords_s.append(np.min(lats))\n",
    "            coords_n.append(np.max(lats))\n",
    "    if coords_w and coords_e and coords_s and coords_n:\n",
    "        return (\n",
    "            float(np.min(coords_w)),\n",
    "            float(np.min(coords_s)),\n",
    "            float(np.max(coords_e)),\n",
    "            float(np.max(coords_n)),\n",
    "        )\n",
    "\n",
    "    return None, None, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4553120",
   "metadata": {},
   "source": [
    "First, let's find all collections that provide netCDF-4 files using the `earthaccess` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ff1b1f-81c5-4857-b88f-e1deb2e0b84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# step 1-a: search collections with netcdf-4\n",
    "\n",
    "query = earthaccess.DataCollections()\n",
    "query.params[\"granule_data_format\"] = \"*netcdf-4*\"\n",
    "query.option(\"granule_data_format\", \"pattern\", True)\n",
    "results = query.get_all()\n",
    "print(f\"Number of collections found: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d37b5",
   "metadata": {},
   "source": [
    "Next, parse metadata for each collection to find a temporal and spatial range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647b41c8-a6da-4f76-8ab4-0ef6048d571d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# step 1-b: parse metadata to find temporal and spatial bounds and save to csv\n",
    "rows = []\n",
    "for rec in results:\n",
    "    meta = rec.get(\"meta\", {}) or {}\n",
    "    umm = rec.get(\"umm\", {}) or {}\n",
    "    concept_id = meta.get(\"concept-id\") or meta.get(\"concept_id\")\n",
    "    short_name = umm.get(\"ShortName\")\n",
    "    entry_title = umm.get(\"EntryTitle\")\n",
    "    provider_id = meta.get(\"provider-id\")\n",
    "\n",
    "    begin, end = _parse_temporal(umm)\n",
    "    west, south, east, north = _parse_bounds_from_spatial(umm)\n",
    "\n",
    "    rows.append(\n",
    "        {\n",
    "            \"concept_id\": concept_id,\n",
    "            \"short_name\": short_name,\n",
    "            \"entry_title\": entry_title,\n",
    "            \"provider_id\": provider_id,\n",
    "            \"begin_time\": begin,\n",
    "            \"end_time\": end,\n",
    "            \"west\": west,\n",
    "            \"south\": south,\n",
    "            \"east\": east,\n",
    "            \"north\": north,\n",
    "        }\n",
    "    )\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "concept_ids = [r[\"concept_id\"] for r in rows if r[\"concept_id\"]]\n",
    "\n",
    "out_csv = \"output/cmr_collections_netcdf4.csv\"\n",
    "df.to_csv(out_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119b002",
   "metadata": {},
   "source": [
    "Next, open a representative netcdf-4 file from each collection and list variable names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bdacbb-f250-4f85-8f29-f4c5ad7c4a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures\n",
    "import earthaccess\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"output/cmr_collections_netcdf4.csv\")\n",
    "\n",
    "for col in [\"links\", \"variables\", \"status\", \"error\", \"scheme\"]:\n",
    "    df[col] = None\n",
    "\n",
    "\n",
    "def _pick_best_link(all_links):\n",
    "    \"\"\"Prefer HTTPS; else S3; else None.\"\"\"\n",
    "    https = [u for u in all_links if u.startswith(\"http\")]\n",
    "    s3 = [u for u in all_links if u.startswith(\"s3://\")]\n",
    "    if s3:\n",
    "        return s3[0]\n",
    "    if https:\n",
    "        return https[0]\n",
    "    return None\n",
    "\n",
    "\n",
    "def _open_xarray_dataset(url):\n",
    "    \"\"\"Open a NetCDF URL that may be HTTPS or S3 and return (ds, scheme).\"\"\"\n",
    "    scheme = urlparse(url).scheme.lower()\n",
    "    if scheme in (\"http\", \"https\"):\n",
    "        fs = earthaccess.get_fsspec_https_session()\n",
    "        return xr.open_dataset(\n",
    "            fs.open(url), engine=\"h5netcdf\", decode_times=False\n",
    "        ), \"https\"\n",
    "    elif scheme == \"s3\":\n",
    "        s3 = earthaccess.get_s3fs_session()\n",
    "        return xr.open_dataset(\n",
    "            s3.open(url, \"rb\"), engine=\"h5netcdf\", decode_times=False\n",
    "        ), \"s3\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported URL scheme: {scheme}\")\n",
    "\n",
    "\n",
    "def process_row(i_row):\n",
    "    i, row = i_row\n",
    "    concept_id = row[\"concept_id\"]\n",
    "    begin = row[\"begin_time\"]\n",
    "    end = (\n",
    "        row[\"end_time\"]\n",
    "        if pd.notna(row[\"end_time\"])\n",
    "        else datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    )\n",
    "\n",
    "    logs = []\n",
    "    logs.append(f\"\\nüîç [{i}] Concept ID: {concept_id}\")\n",
    "    logs.append(f\"   üöÄ [{i}] Starting search for {concept_id}...\")\n",
    "\n",
    "    try:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as ex:\n",
    "            fut = ex.submit(\n",
    "                earthaccess.search_data,\n",
    "                concept_id=concept_id,\n",
    "                temporal=(begin, end),\n",
    "                count=1,\n",
    "            )\n",
    "            results = fut.result(timeout=120)\n",
    "    except concurrent.futures.TimeoutError:\n",
    "        logs.append(f\"   ‚è≥ [{i}] Timeout while searching {concept_id}\")\n",
    "        return {\n",
    "            \"i\": i,\n",
    "            \"concept_id\": concept_id,\n",
    "            \"links\": None,\n",
    "            \"variables\": None,\n",
    "            \"status\": \"timeout\",\n",
    "            \"error\": None,\n",
    "            \"scheme\": None,\n",
    "            \"logs\": logs,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logs.append(f\"   ‚ùå [{i}] Search failed for {concept_id}: {e}\")\n",
    "        return {\n",
    "            \"i\": i,\n",
    "            \"concept_id\": concept_id,\n",
    "            \"links\": None,\n",
    "            \"variables\": None,\n",
    "            \"status\": \"search_failed\",\n",
    "            \"error\": str(e),\n",
    "            \"scheme\": None,\n",
    "            \"logs\": logs,\n",
    "        }\n",
    "\n",
    "    if not results:\n",
    "        logs.append(f\"   ‚ö†Ô∏è  [{i}] No granules for {concept_id}\")\n",
    "        return {\n",
    "            \"i\": i,\n",
    "            \"concept_id\": concept_id,\n",
    "            \"links\": None,\n",
    "            \"variables\": None,\n",
    "            \"status\": \"no_granules\",\n",
    "            \"error\": None,\n",
    "            \"scheme\": None,\n",
    "            \"logs\": logs,\n",
    "        }\n",
    "\n",
    "    try:\n",
    "        all_links = results[0].data_links() or []\n",
    "    except Exception as e:\n",
    "        logs.append(f\"   ‚ö†Ô∏è  [{i}] Could not extract data_links: {e}\")\n",
    "        return {\n",
    "            \"i\": i,\n",
    "            \"concept_id\": concept_id,\n",
    "            \"links\": None,\n",
    "            \"variables\": None,\n",
    "            \"status\": \"no_links\",\n",
    "            \"error\": str(e),\n",
    "            \"scheme\": None,\n",
    "            \"logs\": logs,\n",
    "        }\n",
    "\n",
    "    netcdf_url = _pick_best_link(all_links)\n",
    "    if not netcdf_url:\n",
    "        logs.append(f\"   ‚ö†Ô∏è  [{i}] No usable HTTPS/S3 NetCDF links for {concept_id}\")\n",
    "        return {\n",
    "            \"i\": i,\n",
    "            \"concept_id\": concept_id,\n",
    "            \"links\": None,\n",
    "            \"variables\": None,\n",
    "            \"status\": \"no_links\",\n",
    "            \"error\": None,\n",
    "            \"scheme\": None,\n",
    "            \"logs\": logs,\n",
    "        }\n",
    "\n",
    "    logs.append(f\"   üîó [{i}] Link chosen: {netcdf_url}\")\n",
    "\n",
    "    try:\n",
    "        ds, scheme = _open_xarray_dataset(netcdf_url)\n",
    "        with ds:\n",
    "            variables = list(ds.data_vars.keys())\n",
    "        logs.append(f\"   üìä [{i}] Variables ({len(variables)}): {variables}\")\n",
    "        logs.append(f\"   ‚úÖ [{i}] Result: ok, scheme: {scheme}\")\n",
    "        return {\n",
    "            \"i\": i,\n",
    "            \"concept_id\": concept_id,\n",
    "            \"links\": netcdf_url,\n",
    "            \"variables\": variables,\n",
    "            \"status\": \"ok\",\n",
    "            \"error\": None,\n",
    "            \"scheme\": scheme,\n",
    "            \"logs\": logs,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logs.append(f\"   ‚ö†Ô∏è  [{i}] Failed to open dataset: {e}\")\n",
    "        return {\n",
    "            \"i\": i,\n",
    "            \"concept_id\": concept_id,\n",
    "            \"links\": netcdf_url,\n",
    "            \"variables\": [],\n",
    "            \"status\": \"open_failed\",\n",
    "            \"error\": str(e),\n",
    "            \"scheme\": urlparse(netcdf_url).scheme or None,\n",
    "            \"logs\": logs,\n",
    "        }\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Run in parallel\n",
    "# ----------------------------\n",
    "rows = []\n",
    "n = max(10, len(df))\n",
    "print(f\"\\nüöÄ Starting processing of {n} rows...\", flush=True)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    futures = [executor.submit(process_row, item) for item in df.iloc[:n].iterrows()]\n",
    "    for fut in concurrent.futures.as_completed(futures):\n",
    "        res = fut.result()\n",
    "\n",
    "        # Print all logs for this collection at once\n",
    "        for log in res.get(\"logs\", []):\n",
    "            print(log, flush=True)\n",
    "\n",
    "        rows.append({k: v for k, v in res.items() if k != \"logs\"})\n",
    "\n",
    "out = pd.DataFrame(rows).set_index(\"i\").sort_index()\n",
    "\n",
    "# Merge back into original df (preserves all other columns)\n",
    "df.loc[out.index, [\"links\", \"variables\", \"status\", \"error\", \"scheme\"]] = out[\n",
    "    [\"links\", \"variables\", \"status\", \"error\", \"scheme\"]\n",
    "]\n",
    "\n",
    "print(\"\\nüì¶ Merge complete. Sample:\", flush=True)\n",
    "print(df.loc[out.index, [\"concept_id\", \"scheme\", \"links\", \"status\"]].head(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dafeac-19f7-4576-b810-49423f269940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_vars = df.dropna(subset=[\"variables\"])\n",
    "df_valid_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55457811-98dc-4922-9337-a56363f80948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "df.to_csv(\"output/cmr_collections_netcdf4_updated_saved_all.csv\", index=False)\n",
    "print(f\"\\n‚úÖ Updated CSV saved with {df['link'].notna().sum()} links populated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47e879-27f6-4e99-842e-ee3657a11d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For grouped hdf-5 files, it does not use datatree (reason is current mechanics of Titiler-CMR).\n",
    "url = \"https://data.laadsdaac.earthdatacloud.nasa.gov/prod-lads/VNP03IMG/VNP03IMG.A2012019.0000.002.2020318135750.nc\"\n",
    "fs = earthaccess.get_fsspec_https_session()\n",
    "ds = xr.open_datatree(fs.open(url), engine=\"h5netcdf\", decode_times=False)\n",
    "ds"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datacube-guide",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
