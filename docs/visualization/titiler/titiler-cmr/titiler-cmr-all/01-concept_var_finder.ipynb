{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b66bbff-3a5b-49c7-972d-3aa886c72a2c",
   "metadata": {},
   "source": [
    "# \n",
    "### Step 1: search and find collections with netcdf-4 from earthaccess\n",
    "This notebook shows how to use earthaccess to discover NASA Earthdata collections that provide granules in netCDF-4 format. In the next step, it opens a representative netCDF-4 file from each collection to inspect and list the available variable names.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2105efe7-6e6c-4d1c-a713-b0ceabd40ae7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/datacube-guide_2/datacube-guide/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import earthaccess\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "from typing import Dict, Optional, Tuple, List, Any\n",
    "\n",
    "# ----------------------------------------\n",
    "# Helpers to parse metadata from earthaccess\n",
    "# ----------------------------------------\n",
    "\n",
    "def _parse_temporal(umm: Dict[str, Any]) -> Tuple[Optional[str], Optional[str]]:\n",
    "    temporal = umm.get(\"TemporalExtents\", [])\n",
    "    rng = (temporal or [{}])[0].get(\"RangeDateTimes\", [])\n",
    "    begin = (rng or [{}])[0].get(\"BeginningDateTime\")\n",
    "    end   = (rng or [{}])[0].get(\"EndingDateTime\")\n",
    "    return begin, end\n",
    "\n",
    "def _parse_bounds_from_spatial(umm: Dict[str, Any]) -> Tuple[Optional[float], Optional[float], Optional[float], Optional[float]]:\n",
    "    spatial = umm.get(\"SpatialExtent\", {}) or {}\n",
    "    horiz = spatial.get(\"HorizontalSpatialDomain\", {}) or {}\n",
    "    geom = horiz.get(\"Geometry\", {}) or {}\n",
    "\n",
    "    # 1) Bounding rectangles\n",
    "    rects = geom.get(\"BoundingRectangles\") or []\n",
    "    if rects:\n",
    "        wests = [r.get(\"WestBoundingCoordinate\") for r in rects if r]\n",
    "        easts = [r.get(\"EastBoundingCoordinate\") for r in rects if r]\n",
    "        souths = [r.get(\"SouthBoundingCoordinate\") for r in rects if r]\n",
    "        norths = [r.get(\"NorthBoundingCoordinate\") for r in rects if r]\n",
    "        if all(len(lst) > 0 for lst in (wests, easts, souths, norths)):\n",
    "            return float(np.min(wests)), float(np.min(souths)), float(np.max(easts)), float(np.max(norths))\n",
    "\n",
    "    # 2) GPolygons\n",
    "    gpolys = geom.get(\"GPolygons\") or []\n",
    "    coords_w, coords_e, coords_s, coords_n = [], [], [], []\n",
    "    for gp in gpolys:\n",
    "        b = gp.get(\"Boundary\", {})\n",
    "        pts = b.get(\"Points\", [])\n",
    "        lons = [p.get(\"Longitude\") for p in pts if p and p.get(\"Longitude\") is not None]\n",
    "        lats = [p.get(\"Latitude\") for p in pts if p and p.get(\"Latitude\") is not None]\n",
    "        if lons and lats:\n",
    "            coords_w.append(np.min(lons))\n",
    "            coords_e.append(np.max(lons))\n",
    "            coords_s.append(np.min(lats))\n",
    "            coords_n.append(np.max(lats))\n",
    "    if coords_w and coords_e and coords_s and coords_n:\n",
    "        return float(np.min(coords_w)), float(np.min(coords_s)), float(np.max(coords_e)), float(np.max(coords_n))\n",
    "\n",
    "    return None, None, None, None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4553120",
   "metadata": {},
   "source": [
    "First, let's find all collections that provide netCDF-4 files using the `earthaccess` library.\n",
    "```python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "40ff1b1f-81c5-4857-b88f-e1deb2e0b84a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of collections found: 1939\n",
      "CPU times: user 332 ms, sys: 79.4 ms, total: 412 ms\n",
      "Wall time: 4.45 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# step 1-a: search collections with netcdf-4\n",
    "\n",
    "query = earthaccess.DataCollections()\n",
    "query.params[\"granule_data_format\"] = \"*netcdf-4*\"\n",
    "query.option(\"granule_data_format\", \"pattern\", True)\n",
    "results = query.get_all()\n",
    "print(f\"Number of collections found: {len(results)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5d37b5",
   "metadata": {},
   "source": [
    "Next, parse metadata for each collection to find a temporal and spatial range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "647b41c8-a6da-4f76-8ab4-0ef6048d571d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          concept_id              short_name  \\\n",
      "0  C2105092163-LAADS                VNP03IMG   \n",
      "1  C2105091501-LAADS                VNP02IMG   \n",
      "2  C1562021084-LAADS    CLDMSK_L2_VIIRS_SNPP   \n",
      "3  C1964798938-LAADS  CLDMSK_L2_VIIRS_NOAA20   \n",
      "4  C1593392869-LAADS    CLDMSK_L2_MODIS_Aqua   \n",
      "\n",
      "                                         entry_title provider_id  \\\n",
      "0  VIIRS/NPP Imagery Resolution Terrain Corrected...       LAADS   \n",
      "1  VIIRS/NPP Imagery Resolution 6-Min L1B Swath 3...       LAADS   \n",
      "2       VIIRS/Suomi-NPP Cloud Mask 6-Min Swath 750 m       LAADS   \n",
      "3  VIIRS/NOAA20 Cloud Mask and Spectral Test Resu...       LAADS   \n",
      "4           MODIS/Aqua Cloud Mask 5-Min Swath 1000 m       LAADS   \n",
      "\n",
      "                 begin_time end_time   west  south   east  north  \n",
      "0  2012-01-19T00:00:00.000Z     None -180.0  -90.0  180.0   90.0  \n",
      "1  2012-01-19T00:00:00.000Z     None -180.0  -90.0  180.0   90.0  \n",
      "2  2012-03-01T00:00:00.000Z     None -180.0  -90.0  180.0   90.0  \n",
      "3  2012-03-01T00:00:00.000Z     None -180.0  -90.0  180.0   90.0  \n",
      "4  2002-07-04T00:00:00.000Z     None -180.0  -90.0  180.0   90.0  \n",
      "CPU times: user 74.5 ms, sys: 544 Œºs, total: 75.1 ms\n",
      "Wall time: 82.4 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# step 1-b: parse metadata to find temporal and spatial bounds and save to csv\n",
    "rows = []\n",
    "for rec in results:\n",
    "    meta = rec.get(\"meta\", {}) or {}\n",
    "    umm  = rec.get(\"umm\", {}) or {}\n",
    "    concept_id = meta.get(\"concept-id\") or meta.get(\"concept_id\")\n",
    "    short_name = umm.get(\"ShortName\")\n",
    "    entry_title = umm.get(\"EntryTitle\")\n",
    "    provider_id = meta.get(\"provider-id\")\n",
    "\n",
    "    begin, end = _parse_temporal(umm)\n",
    "    west, south, east, north = _parse_bounds_from_spatial(umm)\n",
    "\n",
    "    rows.append({\n",
    "        \"concept_id\": concept_id,\n",
    "        \"short_name\": short_name,\n",
    "        \"entry_title\": entry_title,\n",
    "        \"provider_id\": provider_id,\n",
    "        \"begin_time\": begin,\n",
    "        \"end_time\": end,\n",
    "        \"west\": west,\n",
    "        \"south\": south,\n",
    "        \"east\": east,\n",
    "        \"north\": north,\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "concept_ids = [r[\"concept_id\"] for r in rows if r[\"concept_id\"]]\n",
    "\n",
    "out_csv = \"cmr_collections_netcdf4.csv\"\n",
    "df.to_csv(out_csv, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119b002",
   "metadata": {},
   "source": [
    "### Step 2: open a representative netcdf-4 file from each collection and list variable names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90bdacbb-f250-4f85-8f29-f4c5ad7c4a3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ Starting processing of 1939 rows...\n",
      "\n",
      "üîç [4] Concept ID: C1593392869-LAADS\n",
      "   üöÄ [4] Starting search for C1593392869-LAADS...\n",
      "   üîó [4] Link chosen: https://data.laadsdaac.earthdatacloud.nasa.gov/prod-lads/CLDMSK_L2_MODIS_Aqua/CLDMSK_L2_MODIS_Aqua.A2002185.0000.001.2021142090111.nc\n",
      "   üìä [4] Variables (0): []\n",
      "   ‚úÖ [4] Result: ok, scheme: https\n",
      "\n",
      "üîç [3] Concept ID: C1964798938-LAADS\n",
      "   üöÄ [3] Starting search for C1964798938-LAADS...\n",
      "   üîó [3] Link chosen: https://data.laadsdaac.earthdatacloud.nasa.gov/prod-lads/CLDMSK_L2_VIIRS_NOAA20/CLDMSK_L2_VIIRS_NOAA20.A2018048.0000.001.2021054143020.nc\n",
      "   üìä [3] Variables (0): []\n",
      "   ‚úÖ [3] Result: ok, scheme: https\n",
      "\n",
      "üîç [2] Concept ID: C1562021084-LAADS\n",
      "   üöÄ [2] Starting search for C1562021084-LAADS...\n",
      "   üîó [2] Link chosen: https://data.laadsdaac.earthdatacloud.nasa.gov/prod-lads/CLDMSK_L2_VIIRS_SNPP/CLDMSK_L2_VIIRS_SNPP.A2012061.0000.001.2019070194123.nc\n",
      "   üìä [2] Variables (0): []\n",
      "   ‚úÖ [2] Result: ok, scheme: https\n",
      "\n",
      "üîç [1] Concept ID: C2105091501-LAADS\n",
      "   üöÄ [1] Starting search for C2105091501-LAADS...\n",
      "   üîó [1] Link chosen: https://data.laadsdaac.earthdatacloud.nasa.gov/prod-lads/VNP02IMG/VNP02IMG.A2012019.0000.002.2020318151901.nc\n",
      "   üìä [1] Variables (0): []\n",
      "   ‚úÖ [1] Result: ok, scheme: https\n",
      "\n",
      "üîç [0] Concept ID: C2105092163-LAADS\n",
      "   üöÄ [0] Starting search for C2105092163-LAADS...\n",
      "   üîó [0] Link chosen: https://data.laadsdaac.earthdatacloud.nasa.gov/prod-lads/VNP03IMG/VNP03IMG.A2012019.0000.002.2020318135750.nc\n",
      "   üìä [0] Variables (0): []\n",
      "   ‚úÖ [0] Result: ok, scheme: https\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 105\u001b[39m\n\u001b[32m    104\u001b[39m futures = [executor.submit(process_row, item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m df.iloc[:n].iterrows()]\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfut\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconcurrent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m    106\u001b[39m \u001b[43m    \u001b[49m\u001b[43mres\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfut\u001b[49m\u001b[43m.\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.5-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:243\u001b[39m, in \u001b[36mas_completed\u001b[39m\u001b[34m(fs, timeout)\u001b[39m\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    240\u001b[39m                 \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m) futures unfinished\u001b[39m\u001b[33m'\u001b[39m % (\n\u001b[32m    241\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m waiter.lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.5-linux-x86_64-gnu/lib/python3.13/threading.py:659\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m659\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.5-linux-x86_64-gnu/lib/python3.13/threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m     \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 103\u001b[39m\n\u001b[32m    101\u001b[39m n = \u001b[38;5;28mmax\u001b[39m(\u001b[32m10\u001b[39m, \u001b[38;5;28mlen\u001b[39m(df))\n\u001b[32m    102\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33müöÄ Starting processing of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m rows...\u001b[39m\u001b[33m\"\u001b[39m, flush=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m concurrent.futures.ThreadPoolExecutor(max_workers=\u001b[32m6\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:\n\u001b[32m    104\u001b[39m     futures = [executor.submit(process_row, item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m df.iloc[:n].iterrows()]\n\u001b[32m    105\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m fut \u001b[38;5;129;01min\u001b[39;00m concurrent.futures.as_completed(futures):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.5-linux-x86_64-gnu/lib/python3.13/concurrent/futures/_base.py:647\u001b[39m, in \u001b[36mExecutor.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.5-linux-x86_64-gnu/lib/python3.13/concurrent/futures/thread.py:239\u001b[39m, in \u001b[36mThreadPoolExecutor.shutdown\u001b[39m\u001b[34m(self, wait, cancel_futures)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads:\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m         \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.5-linux-x86_64-gnu/lib/python3.13/threading.py:1094\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1092\u001b[39m     timeout = \u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1094\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_28027/2374456812.py:30: UserWarning: The 'phony_dims' kwarg now defaults to 'access'. Previously 'phony_dims=None' would raise an error. For full netcdf equivalence please use phony_dims='sort'.\n",
      "  return xr.open_dataset(fs.open(url), engine=\"h5netcdf\", decode_times=False), \"https\"\n"
     ]
    }
   ],
   "source": [
    "import concurrent.futures\n",
    "import os\n",
    "import earthaccess\n",
    "from urllib.parse import urlparse\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"cmr_collections_netcdf4.csv\")\n",
    "\n",
    "for col in [\"links\", \"variables\", \"status\", \"error\", \"scheme\"]:\n",
    "    df[col] = None\n",
    "\n",
    "def _pick_best_link(all_links):\n",
    "    \"\"\"Prefer HTTPS; else S3; else None.\"\"\"\n",
    "    https = [u for u in all_links if u.startswith(\"http\")]\n",
    "    s3 = [u for u in all_links if u.startswith(\"s3://\")]\n",
    "    if s3:\n",
    "        return s3[0]\n",
    "    if https:\n",
    "        return https[0]\n",
    "    return None\n",
    "\n",
    "def _open_xarray_dataset(url):\n",
    "    \"\"\"Open a NetCDF URL that may be HTTPS or S3 and return (ds, scheme).\"\"\"\n",
    "    scheme = urlparse(url).scheme.lower()\n",
    "    if scheme in (\"http\", \"https\"):\n",
    "        fs = earthaccess.get_fsspec_https_session()\n",
    "        return xr.open_dataset(fs.open(url), engine=\"h5netcdf\", decode_times=False), \"https\"\n",
    "    elif scheme == \"s3\":\n",
    "        s3 = earthaccess.get_s3fs_session()\n",
    "        return xr.open_dataset(s3.open(url, \"rb\"), engine=\"h5netcdf\", decode_times=False), \"s3\"\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported URL scheme: {scheme}\")\n",
    "\n",
    "def process_row(i_row):\n",
    "    i, row = i_row\n",
    "    concept_id = row[\"concept_id\"]\n",
    "    begin = row[\"begin_time\"]\n",
    "    end = row[\"end_time\"] if pd.notna(row[\"end_time\"]) else datetime.now(timezone.utc).strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "\n",
    "    logs = []\n",
    "    logs.append(f\"\\nüîç [{i}] Concept ID: {concept_id}\")\n",
    "    logs.append(f\"   üöÄ [{i}] Starting search for {concept_id}...\")\n",
    "\n",
    "    try:\n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=1) as ex:\n",
    "            fut = ex.submit(\n",
    "                earthaccess.search_data,\n",
    "                concept_id=concept_id,\n",
    "                temporal=(begin, end),\n",
    "                count=1,\n",
    "            )\n",
    "            results = fut.result(timeout=120)\n",
    "    except concurrent.futures.TimeoutError:\n",
    "        logs.append(f\"   ‚è≥ [{i}] Timeout while searching {concept_id}\")\n",
    "        return {\"i\": i, \"concept_id\": concept_id, \"links\": None, \"variables\": None, \"status\": \"timeout\", \"error\": None, \"scheme\": None, \"logs\": logs}\n",
    "    except Exception as e:\n",
    "        logs.append(f\"   ‚ùå [{i}] Search failed for {concept_id}: {e}\")\n",
    "        return {\"i\": i, \"concept_id\": concept_id, \"links\": None, \"variables\": None, \"status\": \"search_failed\", \"error\": str(e), \"scheme\": None, \"logs\": logs}\n",
    "\n",
    "    if not results:\n",
    "        logs.append(f\"   ‚ö†Ô∏è  [{i}] No granules for {concept_id}\")\n",
    "        return {\"i\": i, \"concept_id\": concept_id, \"links\": None, \"variables\": None, \"status\": \"no_granules\", \"error\": None, \"scheme\": None, \"logs\": logs}\n",
    "    \n",
    "    try:\n",
    "        all_links = results[0].data_links() or []\n",
    "    except Exception as e:\n",
    "        logs.append(f\"   ‚ö†Ô∏è  [{i}] Could not extract data_links: {e}\")\n",
    "        return {\"i\": i, \"concept_id\": concept_id, \"links\": None, \"variables\": None, \"status\": \"no_links\", \"error\": str(e), \"scheme\": None, \"logs\": logs}\n",
    "\n",
    "    netcdf_url = _pick_best_link(all_links)\n",
    "    if not netcdf_url:\n",
    "        logs.append(f\"   ‚ö†Ô∏è  [{i}] No usable HTTPS/S3 NetCDF links for {concept_id}\")\n",
    "        return {\"i\": i, \"concept_id\": concept_id, \"links\": None, \"variables\": None, \"status\": \"no_links\", \"error\": None, \"scheme\": None, \"logs\": logs}\n",
    "\n",
    "    logs.append(f\"   üîó [{i}] Link chosen: {netcdf_url}\")\n",
    "\n",
    "    try:\n",
    "        ds, scheme = _open_xarray_dataset(netcdf_url)\n",
    "        with ds:\n",
    "            variables = list(ds.data_vars.keys())\n",
    "        logs.append(f\"   üìä [{i}] Variables ({len(variables)}): {variables}\")\n",
    "        logs.append(f\"   ‚úÖ [{i}] Result: ok, scheme: {scheme}\")\n",
    "        return {\n",
    "            \"i\": i, \"concept_id\": concept_id, \"links\": netcdf_url, \"variables\": variables,\n",
    "            \"status\": \"ok\", \"error\": None, \"scheme\": scheme, \"logs\": logs\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logs.append(f\"   ‚ö†Ô∏è  [{i}] Failed to open dataset: {e}\")\n",
    "        return {\n",
    "            \"i\": i, \"concept_id\": concept_id, \"links\": netcdf_url, \"variables\": [],\n",
    "            \"status\": \"open_failed\", \"error\": str(e), \"scheme\": urlparse(netcdf_url).scheme or None, \"logs\": logs\n",
    "        }\n",
    "\n",
    "# ----------------------------\n",
    "# Run in parallel\n",
    "# ----------------------------\n",
    "rows = []\n",
    "n = max(10, len(df))\n",
    "print(f\"\\nüöÄ Starting processing of {n} rows...\", flush=True)\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=6) as executor:\n",
    "    futures = [executor.submit(process_row, item) for item in df.iloc[:n].iterrows()]\n",
    "    for fut in concurrent.futures.as_completed(futures):\n",
    "        res = fut.result()\n",
    "        \n",
    "        # Print all logs for this collection at once\n",
    "        for log in res.get(\"logs\", []):\n",
    "            print(log, flush=True)\n",
    "        \n",
    "        rows.append({k: v for k, v in res.items() if k != \"logs\"})\n",
    "\n",
    "out = pd.DataFrame(rows).set_index(\"i\").sort_index()\n",
    "\n",
    "# Merge back into original df (preserves all other columns)\n",
    "df.loc[out.index, [\"links\", \"variables\", \"status\", \"error\", \"scheme\"]] = \\\n",
    "    out[[\"links\", \"variables\", \"status\", \"error\", \"scheme\"]]\n",
    "\n",
    "print(\"\\nüì¶ Merge complete. Sample:\", flush=True)\n",
    "print(df.loc[out.index, [\"concept_id\", \"scheme\", \"links\", \"status\"]].head(), flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dafeac-19f7-4576-b810-49423f269940",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_valid_vars = df.dropna(subset=[\"variables\"])\n",
    "df_valid_vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55457811-98dc-4922-9337-a56363f80948",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save result\n",
    "df.to_csv(\"cmr_collections_netcdf4_updated_saved_all.csv\", index=False)\n",
    "print(f\"\\n‚úÖ Updated CSV saved with {df['link'].notna().sum()} links populated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d47e879-27f6-4e99-842e-ee3657a11d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "## For grouped hdf-5 files, it does not use datatree (reason is current mechanics of Titiler-CMR).   \n",
    "url=\"https://data.laadsdaac.earthdatacloud.nasa.gov/prod-lads/VNP03IMG/VNP03IMG.A2012019.0000.002.2020318135750.nc\"\n",
    "fs = earthaccess.get_fsspec_https_session()\n",
    "ds=xr.open_datatree(fs.open(url), engine=\"h5netcdf\", decode_times=False)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "739ea32e-e400-4980-89de-9f3fbafb2ab6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data_cube2",
   "language": "python",
   "name": "data_cube2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
